{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.**\n",
        "\n",
        "Answer: Ensemble Learning is a machine learning technique in which multiple individual models (called base learners) are trained and combined to solve the same problem. Instead of relying on a single model, ensemble learning aggregates the predictions of several models to produce a final prediction.\n",
        "\n",
        "The key idea behind ensemble learning is that a group of weak or moderately accurate models, when combined appropriately, can produce better performance, higher accuracy, and more robust predictions than any single model. Ensemble methods reduce errors caused by bias, variance, or noise in the data, depending on the technique used.\n",
        "\n",
        "**Question 2: What is the difference between Bagging and Boosting?**\n",
        "\n",
        "Answer: Bagging (Bootstrap Aggregating) and Boosting are both ensemble techniques but differ in how models are trained and combined.\n",
        "\n",
        "Bagging trains multiple models independently on different bootstrap samples of the dataset and combines their predictions, usually by averaging or majority voting. It mainly aims to reduce variance and is effective for high-variance models like decision trees.\n",
        "\n",
        "Boosting trains models sequentially, where each new model focuses more on the mistakes made by previous models. It aims to reduce both bias and variance by giving more importance to misclassified data points. Examples include AdaBoost and Gradient Boosting.\n",
        "\n",
        "**Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?**\n",
        "\n",
        "Answer: Bootstrap sampling is a technique where multiple datasets are created by randomly sampling from the original dataset with replacement. Each bootstrap sample has the same size as the original dataset, but some data points may appear multiple times while others may be left out.\n",
        "\n",
        "In Bagging methods like Random Forest, bootstrap sampling allows each decision tree to be trained on a slightly different dataset. This diversity among trees reduces correlation between them and helps improve generalization by reducing overfitting.\n",
        "\n",
        "**Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?**\n",
        "\n",
        "Answer: Out-of-Bag (OOB) samples are the data points that are not selected in a particular bootstrap sample during training. On average, about 36 percent of the original data is left out of each bootstrap sample.\n",
        "\n",
        "The OOB score is calculated by using these left-out samples to evaluate the model’s performance. Since OOB samples were not used during training, they act as a validation set. OOB score provides an unbiased estimate of model accuracy without requiring a separate validation dataset.\n",
        "\n",
        "**Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.**\n",
        "\n",
        "Answer: In a single Decision Tree, feature importance is based on how much each feature reduces impurity (such as Gini index or entropy) across splits. However, the importance values can be unstable and highly dependent on the specific training data.\n",
        "\n",
        "In a Random Forest, feature importance is averaged across many decision trees. This makes the importance scores more reliable and robust, as they reflect the contribution of each feature across multiple models rather than a single tree."
      ],
      "metadata": {
        "id": "wRpwUkO-NwuN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoU82mRGNuko",
        "outputId": "ab00260d-0852-426e-bb96-1549223b68ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ],
      "source": [
        "# Question 6: Python program – Random Forest on Breast Cancer dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Create DataFrame and sort\n",
        "df_importance = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print top 5 features\n",
        "print(df_importance.head(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Python program – Bagging Classifier vs Decision Tree on Iris dataset\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# Bagging Classifier\n",
        "bag = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag.fit(X_train, y_train)\n",
        "bag_pred = bag.predict(X_test)\n",
        "bag_acc = accuracy_score(y_test, bag_pred)\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T66peuXhOkfM",
        "outputId": "47456124-f318-4cba-ad0e-fb1f7270a3cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Python program – Random Forest with GridSearchCV\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define model and parameter grid\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 5, 10]\n",
        "}\n",
        "\n",
        "# Grid Search\n",
        "grid = GridSearchCV(rf, param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Final Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tkgcQ85O0L_",
        "outputId": "f11fea12-52ae-4f6c-a5d0-d34d91127222"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 50}\n",
            "Final Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Python program – Bagging Regressor vs Random Forest Regressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging Regressor\n",
        "bag = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag.fit(X_train, y_train)\n",
        "bag_pred = bag.predict(X_test)\n",
        "bag_mse = mean_squared_error(y_test, bag_pred)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "print(\"Bagging Regressor MSE:\", bag_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rMUPRvKPDXM",
        "outputId": "c8dd177b-795d-41a9-c1c0-b3fc84985bfa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.2572988359842641\n",
            "Random Forest Regressor MSE: 0.2553684927247781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Real-world ensemble learning approach for loan default prediction**\n",
        "\n",
        "Answer: To predict loan default, I would first analyze the dataset to understand feature distributions, class imbalance, and noise. Based on the problem, I would choose between Bagging and Boosting. If overfitting and high variance are the main concerns, I would prefer Bagging methods like Random Forest. If improving predictive accuracy on difficult cases is critical, I would choose Boosting methods such as Gradient Boosting or XGBoost.\n",
        "\n",
        "To handle overfitting, I would limit tree depth, use regularization parameters, and apply cross-validation. I would select decision trees as base models because they capture non-linear relationships and interactions well.\n",
        "\n",
        "Model performance would be evaluated using k-fold cross-validation and metrics such as accuracy, precision, recall, F1-score, and ROC-AUC to ensure reliable performance across different data splits.\n",
        "\n",
        "Ensemble learning improves decision-making in this context by providing more stable and accurate predictions, reducing risk in financial decisions, and improving the institution’s ability to correctly identify potential loan defaulters."
      ],
      "metadata": {
        "id": "s7ByamvLPUf6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9UoHC_NCPRMI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}